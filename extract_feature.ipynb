{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobilefacenet OK\n",
      "Mobilefacenet OK 2\n",
      "VisionTransformer OK\n",
      "irback load OK\n",
      "load_weight 304\n",
      "irback OK\n",
      "layers OK\n",
      "layers OK\n"
     ]
    }
   ],
   "source": [
    "# Read facial image frame & give it to the model\n",
    "from models.PosterV2_7cls import *\n",
    "import torch.nn.parallel\n",
    "import os\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "datapath = r\"/home/hyojinju/Dataset/test\"#r\"/home/hyojinju/Dataset/CAER-S\"\n",
    "batch_size = 1\n",
    "workers = 4\n",
    "checkpoint_path = r\"/home/hyojinju/POSTER_V2/checkpoint/caer-s-model_best.pth\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "model = pyramid_trans_expr2(img_size=224, num_classes=7)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "traindir = os.path.join(datapath, 'train')\n",
    "valdir = os.path.join(datapath, 'valid')\n",
    "\n",
    "test_dataset = datasets.ImageFolder(valdir,\n",
    "                                    transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                                        transforms.ToTensor(),\n",
    "                                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                std=[0.229, 0.224, 0.225]),\n",
    "                                                        ]))\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=workers,\n",
    "                                            pin_memory=True)\n",
    "\n",
    "labels = ['A', 'B', 'C', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O']\n",
    "\n",
    "\n",
    "class RecorderMeter1(object):\n",
    "    \"\"\"Computes and stores the minimum loss value and its epoch index\"\"\"\n",
    "\n",
    "    def __init__(self, total_epoch):\n",
    "        self.reset(total_epoch)\n",
    "\n",
    "    def reset(self, total_epoch):\n",
    "        self.total_epoch = total_epoch\n",
    "        self.current_epoch = 0\n",
    "        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
    "        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
    "\n",
    "    def update(self, output, target):\n",
    "        self.y_pred = output\n",
    "        self.y_true = target\n",
    "\n",
    "    def plot_confusion_matrix(self, cm, title='Confusion Matrix', cmap=plt.cm.binary):\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        y_true = self.y_true\n",
    "        y_pred = self.y_pred\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        xlocations = np.array(range(len(labels)))\n",
    "        plt.xticks(xlocations, labels, rotation=90)\n",
    "        plt.yticks(xlocations, labels)\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        plt.figure(figsize=(12, 8), dpi=120)\n",
    "\n",
    "        ind_array = np.arange(len(labels))\n",
    "        x, y = np.meshgrid(ind_array, ind_array)\n",
    "        for x_val, y_val in zip(x.flatten(), y.flatten()):\n",
    "            c = cm_normalized[y_val][x_val]\n",
    "            if c > 0.01:\n",
    "                plt.text(x_val, y_val, \"%0.2f\" % (c,), color='red', fontsize=7, va='center', ha='center')\n",
    "        # offset the tick\n",
    "        tick_marks = np.arange(len(7))\n",
    "        plt.gca().set_xticks(tick_marks, minor=True)\n",
    "        plt.gca().set_yticks(tick_marks, minor=True)\n",
    "        plt.gca().xaxis.set_ticks_position('none')\n",
    "        plt.gca().yaxis.set_ticks_position('none')\n",
    "        plt.grid(True, which='minor', linestyle='-')\n",
    "        plt.gcf().subplots_adjust(bottom=0.15)\n",
    "\n",
    "        plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "        # show confusion matrix\n",
    "        plt.savefig('./log/confusion_matrix.png', format='png')\n",
    "        # fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
    "        print('Saved figure')\n",
    "        plt.show()\n",
    "\n",
    "    def matrix(self):\n",
    "        target = self.y_true\n",
    "        output = self.y_pred\n",
    "        im_re_label = np.array(target)\n",
    "        im_pre_label = np.array(output)\n",
    "        y_ture = im_re_label.flatten()\n",
    "        # im_re_label.transpose()\n",
    "        y_pred = im_pre_label.flatten()\n",
    "        im_pre_label.transpose()\n",
    "\n",
    "class RecorderMeter(object):\n",
    "    \"\"\"Computes and stores the minimum loss value and its epoch index\"\"\"\n",
    "\n",
    "    def __init__(self, total_epoch):\n",
    "        self.reset(total_epoch)\n",
    "\n",
    "    def reset(self, total_epoch):\n",
    "        self.total_epoch = total_epoch\n",
    "        self.current_epoch = 0\n",
    "        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
    "        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
    "\n",
    "    def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n",
    "        self.epoch_losses[idx, 0] = train_loss * 30\n",
    "        self.epoch_losses[idx, 1] = val_loss * 30\n",
    "        self.epoch_accuracy[idx, 0] = train_acc\n",
    "        self.epoch_accuracy[idx, 1] = val_acc\n",
    "        self.current_epoch = idx + 1\n",
    "\n",
    "    def plot_curve(self, save_path):\n",
    "        title = 'the accuracy/loss curve of train/val'\n",
    "        dpi = 80\n",
    "        width, height = 1800, 800\n",
    "        legend_fontsize = 10\n",
    "        figsize = width / float(dpi), height / float(dpi)\n",
    "\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n",
    "        y_axis = np.zeros(self.total_epoch)\n",
    "\n",
    "        plt.xlim(0, self.total_epoch)\n",
    "        plt.ylim(0, 100)\n",
    "        interval_y = 5\n",
    "        interval_x = 5\n",
    "        plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n",
    "        plt.yticks(np.arange(0, 100 + interval_y, interval_y))\n",
    "        plt.grid()\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.xlabel('the training epoch', fontsize=16)\n",
    "        plt.ylabel('accuracy', fontsize=16)\n",
    "\n",
    "        y_axis[:] = self.epoch_accuracy[:, 0]\n",
    "        plt.plot(x_axis, y_axis, color='g', linestyle='-', label='train-accuracy', lw=2)\n",
    "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
    "\n",
    "        y_axis[:] = self.epoch_accuracy[:, 1]\n",
    "        plt.plot(x_axis, y_axis, color='y', linestyle='-', label='valid-accuracy', lw=2)\n",
    "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
    "\n",
    "        y_axis[:] = self.epoch_losses[:, 0]\n",
    "        plt.plot(x_axis, y_axis, color='g', linestyle=':', label='train-loss-x30', lw=2)\n",
    "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
    "\n",
    "        y_axis[:] = self.epoch_losses[:, 1]\n",
    "        plt.plot(x_axis, y_axis, color='y', linestyle=':', label='valid-loss-x30', lw=2)\n",
    "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
    "\n",
    "        if save_path is not None:\n",
    "            fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
    "            print('Saved figure')\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint '/home/hyojinju/POSTER_V2/checkpoint/caer-s-model_best.pth'\n",
      "best_acc:93.0068588256836\n",
      "=> loaded checkpoint '/home/hyojinju/POSTER_V2/checkpoint/caer-s-model_best.pth' (epoch 181)\n",
      "(147, 768) 0\n",
      "(147, 768) 0\n",
      "(147, 768) 0\n",
      "(147, 768) 0\n",
      "(147, 768) 0\n",
      "(147, 768) 0\n",
      "(147, 768) 0\n",
      "(147, 768) 1\n",
      "(147, 768) 1\n",
      "(147, 768) 1\n",
      "(147, 768) 1\n",
      "(147, 768) 1\n",
      "(147, 768) 1\n",
      "(147, 768) 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 2.1857016 , -4.6753135 ,  1.9146696 , ..., -3.053631  ,\n",
       "         -2.1800702 ,  1.8267777 ],\n",
       "        [ 4.5367804 , -6.880458  ,  3.6301682 , ..., -4.810457  ,\n",
       "         -3.3279278 ,  2.703135  ],\n",
       "        [ 4.4297442 , -8.695255  ,  3.3854532 , ..., -5.882805  ,\n",
       "         -3.3752832 ,  4.076563  ],\n",
       "        ...,\n",
       "        [ 0.09259506, -0.37725556,  0.53169596, ..., -0.6998611 ,\n",
       "         -0.6563517 , -0.00916411],\n",
       "        [-0.1701906 , -0.72928655,  0.2515726 , ..., -0.03692304,\n",
       "         -0.12921822, -0.64030516],\n",
       "        [-0.053938  , -0.6623482 ,  0.3417162 , ..., -1.1521099 ,\n",
       "         -0.94193435, -0.38627896]], dtype=float32),\n",
       " array(1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(checkpoint_path):\n",
    "    print(\"=> loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    vit_keys = [\"module.VIT.cls_token\", \"module.VIT.pos_embed\", \"module.VIT.se_block.linear1.weight\", \"module.VIT.se_block.linear1.bias\", \"module.VIT.se_block.linear2.weight\", \"module.VIT.se_block.linear2.bias\", \"module.VIT.patch_embed.proj.weight\", \"module.VIT.patch_embed.proj.bias\", \"module.VIT.head.linear.weight\", \"module.VIT.head.linear.bias\", \"module.VIT.eca_block.conv.weight\", \"module.VIT.CON1.weight\", \"module.VIT.IRLinear1.weight\", \"module.VIT.IRLinear1.bias\", \"module.VIT.IRLinear2.weight\", \"module.VIT.IRLinear2.bias\", \"module.VIT.blocks.0.norm1.weight\", \"module.VIT.blocks.0.norm1.bias\", \"module.VIT.blocks.0.conv.weight\", \"module.VIT.blocks.0.conv.bias\", \"module.VIT.blocks.0.attn.qkv.weight\", \"module.VIT.blocks.0.attn.qkv.bias\", \"module.VIT.blocks.0.attn.proj.weight\", \"module.VIT.blocks.0.attn.proj.bias\", \"module.VIT.blocks.0.norm2.weight\", \"module.VIT.blocks.0.norm2.bias\", \"module.VIT.blocks.0.mlp.fc1.weight\", \"module.VIT.blocks.0.mlp.fc1.bias\", \"module.VIT.blocks.0.mlp.fc2.weight\", \"module.VIT.blocks.0.mlp.fc2.bias\", \"module.VIT.blocks.1.norm1.weight\", \"module.VIT.blocks.1.norm1.bias\", \"module.VIT.blocks.1.conv.weight\", \"module.VIT.blocks.1.conv.bias\", \"module.VIT.blocks.1.attn.qkv.weight\", \"module.VIT.blocks.1.attn.qkv.bias\", \"module.VIT.blocks.1.attn.proj.weight\", \"module.VIT.blocks.1.attn.proj.bias\", \"module.VIT.blocks.1.norm2.weight\", \"module.VIT.blocks.1.norm2.bias\", \"module.VIT.blocks.1.mlp.fc1.weight\", \"module.VIT.blocks.1.mlp.fc1.bias\", \"module.VIT.blocks.1.mlp.fc2.weight\", \"module.VIT.blocks.1.mlp.fc2.bias\", \"module.VIT.norm.weight\", \"module.VIT.norm.bias\"]\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    for key in vit_keys:\n",
    "        state_dict.pop(key, None)\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    best_acc = best_acc.to()\n",
    "    print(f'best_acc:{best_acc}')\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(checkpoint_path))\n",
    "\n",
    "\n",
    "def extract_feature(val_loader, model):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    # D = [[0, 0, 0, 0, 0, 0, 0],\n",
    "    #      [0, 0, 0, 0, 0, 0, 0],\n",
    "    #      [0, 0, 0, 0, 0, 0, 0],\n",
    "    #      [0, 0, 0, 0, 0, 0, 0],\n",
    "    #      [0, 0, 0, 0, 0, 0, 0],\n",
    "    #      [0, 0, 0, 0, 0, 0, 0],\n",
    "    #      [0, 0, 0, 0, 0, 0, 0]]\n",
    "    with torch.no_grad():\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "            output = model(images)\n",
    "\n",
    "            target = target.squeeze().cpu().numpy()\n",
    "            output = output.squeeze().cpu().numpy()\n",
    "\n",
    "            print(np.shape(output), target)\n",
    "            # im_re_label = np.array(target)\n",
    "            # im_pre_label = np.array(output)\n",
    "            # y_ture = im_re_label.flatten()\n",
    "            # im_re_label.transpose()\n",
    "            # y_pred = im_pre_label.flatten()\n",
    "            # im_pre_label.transpose()\n",
    "\n",
    "            # C = metrics.confusion_matrix(y_ture, y_pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "            # D += C\n",
    "\n",
    "            # if i % args.print_freq == 0:\n",
    "            #     progress.display(i)\n",
    "\n",
    "        # print(' **** Accuracy {top1.avg:.3f} *** '.format(top1=top1))\n",
    "        # with open('./log/' + time_str + 'log.txt', 'a') as f:\n",
    "        #     f.write(' * Accuracy {top1.avg:.3f}'.format(top1=top1) + '\\n')\n",
    "    # print(D)\n",
    "    return output, target#top1.avg, losses.avg, output, target, D\n",
    "# Save the feature tensor (147x768)\n",
    "\n",
    "extract_feature(val_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poster_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
